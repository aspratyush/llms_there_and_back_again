### Transformers
3 basic concepts : Tokenization -> Embedding -> Positional Encoing -> Inference -> Output Probabilities

#### Usage in LLMs
- As a `BaseModel` : Pretrained on massive datasets
- As a `Fine-Tuned Model` : fine tuned on domain and optionally also includes reinforcement learning from human feedback (e.g : GPT3+)
